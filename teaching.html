<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Teaching</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html">Publication</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching&nbsp;and&nbsp;talks</a></div>
<div class="menu-item"><a href="publications/resume_misiakiewicz.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Teaching</h1>
</div>
<ul>
<li><p>Simon's institute (Fall 2021): organizing a reading group on the <i>Mean-Field description of neural networks</i> (<a href="https://simons.berkeley.edu/events/gmos-working-group-mean-field-nn">link</a>). (<a href="publications/MFNN_session1_Overview.pdf">Handwritten notes</a>).</p>
</li>
</ul>
<ul>
<li><p>Teaching assistant at the Deep Learning Theory Summer School, Princeton, summer 2021 (<a href="https://deep-learning-summer-school.princeton.edu">link</a>).</p>
<ul>
<li><p>Session 1: <i>Implicit regularization</i> (<a href="publications/TAsession_1_Implicit_Regularization.pdf">handwritten notes</a>).</p>
</li>
<li><p>Session 2: <i>Benign overfitting</i> (<a href="publications/TA_session_2_Benign_overfitting.pdf">handwritten notes</a>).</p>
</li>
<li><p>Session 3: <i>Going beyond the linear regime</i> (<a href="publications/TA_session_3_beyond_linear_regime.pdf">handwritten notes</a>).</p>
</li>
<li><p>Lecture notes: <i>An introduction to the analysis of linearized neural networks</i> with A. Montanari (<a href="publications/linear-nets.pdf">link soon</a>).</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>NSF collaboration reading group: <a href="publications/NSF_notes.pdf">handwritten notes</a> on breaking the curse of dimensionality with neural networks.</p>
</li>
</ul>
<ul>
<li><p>EE375: <a href="publications/EE375-Lect13.pdf">handwritten notes</a> on generalization error of random features models.</p>
</li>
</ul>
<ul>
<li><p>Teaching assistant in the Statistics Department, Stanford (2017-2022)</p>
<ul>
<li><p>STATS 310A: Theory of Probability I (Fall 2021).</p>
</li>
<li><p>MATH 20: Introduction to calculus (Summer 2021).</p>
</li>
<li><p>STATS 375: Mathematical problems in Machine Learning (Spring 2021).</p>
</li>
<li><p>STATS 300C: Theory of statistics III (Spring 2021).</p>
</li>
<li><p>STATS 216: Introduction to Statistical Learning (Winter 2021).</p>
</li>
<li><p>STATS 200: Introduction to Statistical Inference (Fall 2020, Winter 2018).</p>
</li>
<li><p>STATS 116: Theory of Probability (Spring 2020, Fall 2018).</p>
</li>
<li><p>STATS 221: Random Processes on Graphs and Lattices (Winter 2020).</p>
</li>
<li><p>STATS 110: Statistical Methods in Engineering and the Physical Sciences (Fall 2019).</p>
</li>
<li><p>STATS 310C: Theory of Probability III (Spring 2019).</p>
</li>
<li><p>STATS 202: Data Mining and Analysis (Summer 2018).</p>
</li>
<li><p>STATS 101: Data Science 101 (Fall 2017).</p>
</li>
</ul>

</li>
</ul>
<h1>Talks</h1>
<ul>
<li><p><i>Mean-Field description of two-layers neural networks</i>, Simons Institute, Berkeley (October 2021). Workshop <a href="https://simons.berkeley.edu/workshops/gmos2021-2">Dynamics and Discretization: PDEs, Sampling, and Optimization</a>, part of the program <a href="https://simons.berkeley.edu/programs/gmos2021">Geometric Methods in Optimization and Sampling</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning with invariances in random features and kernel models</i>. Conference of Learning Theory (August 2021). <a href="http://www.learningtheory.org/colt2021/virtual/poster_1209.html">Video</a>. <a href="publications/InvarianceCOLT_slides.pdf">Slides</a>. <a href="publications/InvarianceCOLT_poster.pdf">Poster</a>. </p>
</li>
</ul>
<ul>
<li><p><i>Minimum Complexity Interpolation in Random Features Models</i>. Youth in High-Dimensions, Trieste (June 2021). <a href="http://indico.ictp.it/event/9596/session/16/contribution/62/material/video/">Video</a>. <a href="publications/PresentationTrieste.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning structured data with random features and kernel methods</i>. ML lunch, Stanford (April 2021). <a href="publications/Presentation_MLlunch.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning structured data with random features and kernel methods</i>. NSF-Simons Journal Club (April 2021). </p>
</li>
</ul>
<ul>
<li><p><i>When Do Neural Networks Outperform Kernel Methods?</i> G-Research (February 2021). </p>
</li>
</ul>
<ul>
<li><p><i>When Do Neural Networks Outperform Kernel Methods?</i> NeurIPS 2020. <a href="https://crossminds.ai/video/when-do-neural-networks-outperform-kernel-methods-606ff214f43a7f2f827c1a8d/">Video</a>. <a href="publications/Anisotropic_Poster_NeurIPS.pdf">Poster</a>.</p>
</li>
</ul>
<ul>
<li><p>ML Foundations seminar, Microsoft Research (November 2020). </p>
</li>
</ul>
<ul>
<li><p>MoDL (NSF collaboration) (October 2020). </p>
</li>
</ul>
<ul>
<li><p><i>Limitations of Lazy-Training in two-layers neural networks</i>. NeurIPS 2019. <a href="publications/NeurIPS_Limitations.pdf">Slides</a>. <a href="publications/Poster_NeurIPS_2019.pdf">Poster</a>.</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
