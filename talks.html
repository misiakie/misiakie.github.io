<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tutorials</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Info</div>
<div class="menu-item"><a href="index.html">Main</a></div>
<div class="menu-item"><a href="bio.html">Short&nbsp;bio</a></div>
<div class="menu-item"><a href="publications/CV_Misiakiewicz.pdf">CV</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="talks.html" class="current">Talks/Slides</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="teaching.html">List</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Tutorials</h1>
</div>
<ul>
<li><p>Lecture notes with Andrea Montanari: <a href="https://arxiv.org/pdf/2308.13431">Six lectures on linearized neural networks</a>. Based on lectures given by A. Montanari at the &ldquo;Deep Learning Theory Summer School&rdquo;, Princeton (2021), and at the summer school &ldquo;Statistical Physics &amp; Machine Learning&rdquo;, Les Houches School of Physics (2022).</p>
</li>
</ul>
<ul>
<li><p>Simon's institute (Fall 2021): organizing a reading group on the <i>Mean-Field description of neural networks</i> (<a href="https://simons.berkeley.edu/events/gmos-working-group-mean-field-nn">link</a>). (<a href="publications/MFNN_session1_Overview.pdf">Handwritten notes</a>).</p>
</li>
</ul>
<ul>
<li><p>Teaching assistant at the Deep Learning Theory Summer School, Princeton, summer 2021 (<a href="https://deep-learning-summer-school.princeton.edu">link</a>).</p>
<ul>
<li><p>Session 1: <i>Implicit regularization</i> (<a href="publications/TAsession_1_Implicit_Regularization.pdf">handwritten notes</a>).</p>
</li>
<li><p>Session 2: <i>Benign overfitting</i> (<a href="publications/TA_session_2_Benign_overfitting.pdf">handwritten notes</a>).</p>
</li>
<li><p>Session 3: <i>Going beyond the linear regime</i> (<a href="publications/TA_session_3_beyond_linear_regime.pdf">handwritten notes</a>).</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>NSF collaboration reading group: <a href="publications/NSF_notes.pdf">handwritten notes</a> on breaking the curse of dimensionality with neural networks.</p>
</li>
</ul>
<ul>
<li><p>EE375: <a href="publications/EE375-Lect13.pdf">handwritten notes</a> on generalization error of random features models.</p>
</li>
</ul>
<h1>Talks, Workshops and Conferences</h1>
<ul>
<li><p>JSM, Nashville (August, 2025).</p>
</li>
</ul>
<ul>
<li><p>Random matrices and high-dimensional learning dynamics, CRM, Montreal (June 2025).</p>
</li>
</ul>
<ul>
<li><p>Statistics Seminar, Cornell (November 2024).</p>
</li>
</ul>
<ul>
<li><p>Wilks Memorial Seminar, Princeton (October 2024).</p>
</li>
</ul>
<ul>
<li><p>INFORMS, Seattle (October 2024).</p>
</li>
</ul>
<ul>
<li><p>Big Data and Artificial Intelligence in Econometrics, Finance, and Statistics, Stevanovich Center, UChicago (October 2024).</p>
</li>
</ul>
<ul>
<li><p>PDE Methods in Machine Learning: from Continuum Dynamics to Algorithms, Granada, Spain (June 2024).</p>
</li>
</ul>
<ul>
<li><p>Workshop on Statistical Inference and Learning Dynamics, IDEAL, Northwestern (May 2024).</p>
</li>
</ul>
<ul>
<li><p>MoDL meeting, San Diego (May 2024).</p>
</li>
</ul>
<ul>
<li><p>Random Structures, Computation, and Statistical Inference, AMS sectional meeting, San Francisco (May 2024).</p>
</li>
</ul>
<ul>
<li><p>Research at TTIC seminar, TTIC (April 2024).</p>
</li>
</ul>
<ul>
<li><p>NSF TRIPODS II Meeting (April 2024).</p>
</li>
</ul>
<ul>
<li><p><a href="https://cargese2023.github.io">Statistical physics &amp; machine learning</a>, Cargese (August 2023).</p>
</li>
</ul>
<ul>
<li><p>Workshop on <a href="http://perso.ens-lyon.fr/justin.ko/porquerolles2023.html">High Dimensional Statistics and Random Matrices</a>, Porquerolles (June 2023).</p>
</li>
</ul>
<ul>
<li><p><i>Learning latent low-dimensional functions with neural networks</i>, <a href="https://www.ias.tum.de/ias/event-pages/workshop-otmfml/program/">&ldquo;Optimal Transport, Mean-Field Models, and Machine Learning&rdquo;</a> at TUM-IAS Munich, Germany (April 2023). <a href="https://www.ias.tum.de/fileadmin/w00bub/www/_my_direct_uploads/Slides_Misiakiewicz.pdf">Slides</a></p>
</li>
</ul>
<ul>
<li><p>Statistics seminar, Yale University (April 2023).</p>
</li>
</ul>
<ul>
<li><p><i>New statistical and computational phenomena from Deep Learning</i>, TTIC, Chicago (January 2023).</p>
</li>
</ul>
<ul>
<li><p>Statistics seminar, University of Chicago (January 2023).</p>
</li>
</ul>
<ul>
<li><p><i>Learning with convolution and pooling operations in kernel methods</i>, NeurIPS 2022 (December 2022).</p>
</li>
</ul>
<ul>
<li><p><a href="https://mad.cds.nyu.edu/seminar/">Math and Data seminar</a>, Center for Data Science and Courant Institute, NYU (November 2022).</p>
</li>
</ul>
<ul>
<li><p>ML theory seminar (alg-ml) at Princeton (November 2022).</p>
</li>
</ul>
<ul>
<li><p><i>Learning Sparse functions with SGD on neural networks</i>, Informs 2022, Topics in Theory of Neural Networks (October 2022).</p>
</li>
</ul>
<ul>
<li><p><i>Computational aspects of learning sparse functions with neural networks</i>,  Information Systems Laboratory (ISL) Colloquium, Stanford (October 2022).</p>
</li>
</ul>
<ul>
<li><p><i>Breaking the curse of dimensionality with neural networks</i>, <a href="https://memento.epfl.ch/event/external-flair-seminar-theodor-misiakiewicz-2/">External FLAIR seminar</a>, EPFL (September 2022). <a href="FlairEPFL_Sept2022_Misiakiewicz.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks</i>, COLT 2022 (July 2022). With the amazing <a href="http://web.mit.edu/eboix/www/">Enric Boix-Adsera</a>. <a href="publications/PresentationCOLT2022_MergedStaircase.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning sparse functions in the mean-field regime</i>, Learning: Optimization and Stochastics (Summer Research Institute 2022), EPFL (June 2022). <a href="publications/SparseFunction_EPFL_Misiakiewicz.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>When do neural networks outperform kernel methods? Are kernel methods doomed?</i>, ELLIS reading group on Mathematics of Deep Learning (June 2022). <a href="publications/EllisPresentation_Misiakiewicz.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Tutorial: Benign Overfitting, Double Descent and RKHS ridge regression made easy!</i>, New Interactions Between Statistics and Optimization workshop, BIRS, Banff (May 2022). <a href="publications/PresentationBanff_Misiakiewicz.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning staircases: deep learning takes a staircase to heaven</i>, Deep Learning Theory Symposium workshop, Simons Institute, Berkeley (December 2021). <a href="https://www.youtube.com/watch?v=PeSFhplFkL4">Video</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Mean-Field description of two-layers neural networks</i>, Simons Institute, Berkeley (October 2021). Workshop <a href="https://simons.berkeley.edu/workshops/gmos2021-2">Dynamics and Discretization: PDEs, Sampling, and Optimization</a>, part of the program <a href="https://simons.berkeley.edu/programs/gmos2021">Geometric Methods in Optimization and Sampling</a>. <a href="https://simons.berkeley.edu/talks/mean-field-theory-two-layers-neural-networks-dimension-free-bounds-and-example">Video</a>. <a href="publications/Misiakiewicz_MeanFieldNNs_Simons.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning with invariances in random features and kernel models</i>. Conference of Learning Theory (August 2021). <a href="http://www.learningtheory.org/colt2021/virtual/poster_1209.html">Video</a>. <a href="publications/InvarianceCOLT_slides.pdf">Slides</a>. <a href="publications/InvarianceCOLT_poster.pdf">Poster</a>. </p>
</li>
</ul>
<ul>
<li><p><i>Minimum Complexity Interpolation in Random Features Models</i>. Youth in High-Dimensions, Trieste (June 2021). <a href="http://indico.ictp.it/event/9596/session/16/contribution/62/material/video/">Video</a>. <a href="publications/PresentationTrieste.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning structured data with random features and kernel methods</i>. ML lunch, Stanford (April 2021). <a href="publications/Presentation_MLlunch.pdf">Slides</a>.</p>
</li>
</ul>
<ul>
<li><p><i>Learning structured data with random features and kernel methods</i>. NSF-Simons Journal Club (April 2021). </p>
</li>
</ul>
<ul>
<li><p><i>When Do Neural Networks Outperform Kernel Methods?</i> G-Research (February 2021). </p>
</li>
</ul>
<ul>
<li><p><i>When Do Neural Networks Outperform Kernel Methods?</i> NeurIPS 2020. <a href="https://crossminds.ai/video/when-do-neural-networks-outperform-kernel-methods-606ff214f43a7f2f827c1a8d/">Video</a>. <a href="publications/Anisotropic_Poster_NeurIPS.pdf">Poster</a>.</p>
</li>
</ul>
<ul>
<li><p>ML Foundations seminar, Microsoft Research (November 2020). </p>
</li>
</ul>
<ul>
<li><p>MoDL (NSF collaboration) (October 2020). </p>
</li>
</ul>
<ul>
<li><p><i>Limitations of Lazy-Training in two-layers neural networks</i>. NeurIPS 2019. <a href="publications/NeurIPS_Limitations.pdf">Slides</a>. <a href="publications/Poster_NeurIPS_2019.pdf">Poster</a>.</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
