<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>S&amp;DS 659: Mathematics of Deep Learning</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Info</div>
<div class="menu-item"><a href="index.html">Main</a></div>
<div class="menu-item"><a href="bio.html">Short&nbsp;bio</a></div>
<div class="menu-item"><a href="publications/CV_Misiakiewicz.pdf">CV</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks/Slides</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="teaching.html">List</a></div>
<div class="menu-item"><a href="SDS659.html" class="current">S&amp;DS&nbsp;659</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>S&amp;DS 659: Mathematics of Deep Learning</h1>
</div>
<h1>Description </h1>
<p>The goal of this course is to provide an introduction to selected topics in deep learning theory. I will present a number of mathematical models and theoretical concepts that have emerged in recent years to understand neural networks.<br />
<br />
<b>Lectures:</b> Wednesdays 4:00pm&ndash;5:50pm <br />
<b>Office Hours:</b> Thursdays 4:00pm&ndash;5:00pm, Kline Tower 1049<br />
<br />
<b>Prerequesites:</b> I will not assume specific background in machine learning, let alone neural networks. On the other hand, I will assume a degree of mathematical maturity, in particular in linear algebra, analysis, and probability theory (at the level of S&amp;DS 241/541).<br />
<br />
<b>Assignments:</b>  Scribe one lecture during the semester. You will have to write a report on a research topic related to deep learning theory, and give a presentation at the end of the semester.</p>
<h1>Course syllabus</h1>
<ul>
<li><p><b>Week 1: General Introduction</b></p>
<ul>
<li><p>Empirical risk minimization and the classical paradigm of statistical learning.</p>
</li>
<li><p>Tractability via overparametrization, implicit bias.</p>
</li>
<li><p>Universal approximation, Barron's theorem, uniform convergence.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 2: Generalization and Uniform Convergence</b></p>
<ul>
<li><p>Basics of uniform convergence theory.</p>
</li>
<li><p>Norm-based uniform convergence for multilayer NNs.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 3: Implicit Bias</b></p>
<ul>
<li><p>Implicit bias of learning algorithms.</p>
</li>
<li><p>Examples of mirror descent and steepest descent.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 4: Benign Overfitting/Double Descent</b></p>
<ul>
<li><p>Overfitting and double descent phenomena.</p>
</li>
<li><p>Benign overfitting in linear regression, self-induced regularization.</p>
</li>
<li><p>Inner-product kernels on the sphere.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 5: Lazy Regime and NTK</b></p>
<ul>
<li><p>Lazy training regime in optimization.</p>
</li>
<li><p>Global convergence of two-layer NNs.</p>
</li>
<li><p>Neural Tangent Kernel.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 6: Kernel Methods</b></p>
<ul>
<li><p>Background on kernel methods.</p>
</li>
<li><p>Deterministic equivalents for ridge regression.</p>
</li>
<li><p>Curse-of-dimensionality, learning lower bounds for linear methods.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 7: Mean-Field Description</b></p>
<ul>
<li><p>Infinite-width limits and mu parametrization.</p>
</li>
<li><p>Mean-field theory for two-layer NNs: McKeanâ€“Vlasov PDE and Optimal transport formulations.</p>
</li>
<li><p>Global convergence guarantees.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 8: Linear Methods vs Feature Selection vs Feature Learning</b></p>
<ul>
<li><p>Convex neural networks.</p>
</li>
<li><p>Case study: multi-index functions.</p>
</li>
<li><p>Staircase mechanism.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 9: High-dimensional Landscapes and Dynamics</b></p>
<ul>
<li><p>Landscape concentration.</p>
</li>
<li><p>Dynamics on non-convex problems in high dimensions.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 10: Power and Limitations of Differentiable Learning</b></p>
<ul>
<li><p>Computational hardness of deep learning.</p>
</li>
<li><p>Poly-time universality of SGD on NNs.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 11: Transformers, Attention, and In-Context Learning</b></p>
<ul>
<li><p>Transformer architecture, attention layer.</p>
</li>
<li><p>In-context linear regression.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 12: Edge-of-Stability, Neural Scaling Laws, Emergence, and Beyond</b></p>
<ul>
<li><p>Review of a number of phenomena.</p>
</li>
<li><p>Open ended discussions.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Week 13: In-class presentations + pizza</b></p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
