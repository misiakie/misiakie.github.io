<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Theodor Misiakiewicz</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="home.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publication</a></div>
<div class="menu-item"><a href="teaching.html">Teaching&nbsp;and&nbsp;talks</a></div>
<div class="menu-item"><a href="publications/CV_Misiakiewicz.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Theodor Misiakiewicz</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://misiakie.github.io"><img src="images/misiakiewicz.jpg" alt="alt text" width="150px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>PhD,<br /> <a href="https://statistics.stanford.edu">Statistics Department</a>, <br /><a href="https://www.stanford.edu">Stanford University</a><br />
390 Serra Mall, Sequoia Hall, <br />
Stanford, CA 94305-9510 <br />
Phone: +<b></b>* 390 7662 <br />
Email: <i>misiakie</i> [@] stanford [DOT] edu</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a sixth-year PhD candidate in Statistics at Stanford University working with <a href="https://web.stanford.edu/~montanar/">Prof. Andrea Montanari</a>. Before my Ph.D, I completed my undergraduate studies in France at Ecole Normale Superieure de Paris with a focus on pure math and physics. I received a B.Sc. in Mathematics and a B.Sc. in Physics in 2014, and a M.Sc. in Theoretical Physics at ICFP in 2016. <br />
<br />
My interest lies broadly at the intersection of statistics, machine learning, probability and computer science. Lately, I have been focusing on the statistical and computational aspects of deep learning, and the performance of kernel and random feature methods in high dimension. Some of the questions I am currently interested in: When can we expect neural networks to outperform kernel methods? When can neural networks beat the curse of dimensionality? On the other hand, what are the computational limits of  gradient-trained neural networks? What structures in real data allows for efficient learning? When is overfitting benign? How much overparametrization is optimal? When can we expect universal or non-universal behavior in empirical risk minimization?  <br />
<br />
Previous research projects led me to work on high-dimensional statistics (inverse Ising problem), non-convex optimization (max-cut, community detection, synchronization), LDPC codes, <a href="https://home.cern/science/experiments/cms">CMS experiment at LHC</a> (detection of Higgs Boson in the 2015 data), <a href="https://www.dwavesys.com">D-Wave 2X</a> (quantum annealer) and massive gravity. <br />
<br /> 
Here is a link to my <a href="https://scholar.google.com/citations?user=E8Jst30AAAAJ&amp;hl=en">google scholar account</a>.</p>
<h2>Research</h2>
<p>My research interests include </p>
<ul>
<li><p>Theory of Deep Learning: mean-field description and neural tangent kernel</p>
</li>
<li><p>Kernel and random feature methods in high-dimension (benign overfitting, multiple descent, structured kernels)</p>
</li>
<li><p>Non-convex optimization, implicit regularization, landscape analysis</p>
</li>
<li><p>Computational limits of learning with neural networks</p>
</li>
<li><p>Random matrix theory, high-dimensional probability</p>
</li>
</ul>
<p><a href="publications.html">Full list of publications</a>.
<br />
<a href="publications/CV_Misiakiewicz.pdf">CV</a> (last updated 2022).</p>
</td>
</tr>
</table>
</body>
</html>
